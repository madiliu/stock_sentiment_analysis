{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cVl28-9KPPt"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e06mJ1RR1VHZ"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TnlNpAj3-gT"
      },
      "source": [
        "### Write to excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMmDdeIkVSt0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "numerical_excel_path = '/content/drive/My Drive/Colab Notebooks/numerical_vTest.xlsx'\n",
        "contextual_excel_path = '/content/drive/My Drive/Colab Notebooks/contextual_vTest.xlsx'\n",
        "\n",
        "\n",
        "def write_to_excel(result, sheet_name, excel_path):\n",
        "    # Define the column names\n",
        "    df = pd.DataFrame(result)\n",
        "    df.columns = ['title', 'sentiment', 'confidence']\n",
        "\n",
        "    with pd.ExcelWriter(excel_path, mode='a', engine='openpyxl', if_sheet_exists='new') as writer:\n",
        "        df.to_excel(writer, sheet_name=sheet_name, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeLNrKLU0jNQ"
      },
      "outputs": [],
      "source": [
        "!ls -l {numerical_excel_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AaCYXQh0n2a"
      },
      "outputs": [],
      "source": [
        "!unzip -t {numerical_excel_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSi_EEaNKVGz"
      },
      "source": [
        "### Reading news data - [Twitter Value](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wDRFDLjotUK"
      },
      "outputs": [],
      "source": [
        "numerical_filename = 'Numerical.xlsx'\n",
        "search_path = '/content/drive/My Drive/Colab Notebooks'\n",
        "\n",
        "file_path = None\n",
        "\n",
        "# read numerical\n",
        "for root, dirs, files in os.walk(search_path):\n",
        "    if numerical_filename in files:\n",
        "        file_path = os.path.join(root, numerical_filename)\n",
        "        break\n",
        "        numerical_df = pd.read_excel(file_path)\n",
        "\n",
        "numerical_df = pd.read_excel(file_path)\n",
        "# numerical_df.head()\n",
        "\n",
        "numerical_titles = numerical_df.iloc[:, 0]\n",
        "numerical_titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmx0w3-rrzHo"
      },
      "outputs": [],
      "source": [
        "contextual_filename = 'Contextual.xlsx'\n",
        "search_path = '/content/drive/My Drive/Colab Notebooks'\n",
        "\n",
        "file_path = None\n",
        "\n",
        "# read contextual\n",
        "for root, dirs, files in os.walk(search_path):\n",
        "    if contextual_filename in files:\n",
        "        file_path = os.path.join(root, contextual_filename)\n",
        "        break\n",
        "    # contextual_df = pd.read_excel(file_path)\n",
        "contextual_df = pd.read_excel(file_path)\n",
        "\n",
        "contextual_titles = contextual_df.iloc[:, 0]\n",
        "contextual_titles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE8091eC4fxC"
      },
      "source": [
        "## Sentiment analysis function for each models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyUh2f0TNdH8"
      },
      "source": [
        "### 1. [TextBlob](https://github.com/sloria/TextBlob) - NaiveBayesAnalyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXcTA1byzX5J"
      },
      "source": [
        "`NaiveBayesAnalyzer`: uses [`nltk`](https://github.com/nltk/nltk), this model was trained on a movie review corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1kV19Ajt75E"
      },
      "outputs": [],
      "source": [
        "!python -m textblob.download_corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqFuWLdqud7G"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "from textblob.sentiments import NaiveBayesAnalyzer\n",
        "\n",
        "def textblob(titles, excel_path):\n",
        "  excel_result = []\n",
        "\n",
        "  start_time = time.time()\n",
        "  for title in titles:\n",
        "      blob = TextBlob(title, analyzer=NaiveBayesAnalyzer())\n",
        "      # print(title)\n",
        "      # print(blob.sentiment)\n",
        "      # print(\"-----------------------\")\n",
        "      # Unpack the sentiment into its own tuple\n",
        "      sentiment_data = (blob.sentiment.classification, blob.sentiment.p_pos, blob.sentiment.p_neg)\n",
        "      # Extend the original tuple (title, sentiment) with the unpacked sentiment data\n",
        "      excel_result.append((title,) + sentiment_data)\n",
        "\n",
        "  end_time = time.time()\n",
        "  duration = end_time - start_time\n",
        "\n",
        "  # The rest of your code remains the same\n",
        "  df = pd.DataFrame(excel_result, columns=['title', 'classification', 'p_pos', 'p_neg'])\n",
        "\n",
        "  # Now you can write df to the Excel file as before\n",
        "  with pd.ExcelWriter(excel_path, mode='a', engine='openpyxl', if_sheet_exists='new') as writer:\n",
        "      df.to_excel(writer, sheet_name=\"TextBlob\")\n",
        "\n",
        "  return duration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-O6AdVxQE1A"
      },
      "source": [
        "### 2. [VADER](https://github.com/cjhutto/vaderSentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBm0SHbtQiMZ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from zipfile import BadZipFile\n",
        "\n",
        "custom_nltk_path = '/content/drive/My Drive/Colab Notebooks/nltk_data'\n",
        "nltk.data.path.append(custom_nltk_path)\n",
        "\n",
        "try:\n",
        "    nltk.download('vader_lexicon', force=True)\n",
        "except BadZipFile as e:\n",
        "    print(f\"Failed to open: {e.filename}\")\n",
        "\n",
        "\n",
        "def vader(titles, excel_path):\n",
        "\n",
        "  # Initialize the VADER sentiment intensity analyzer\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "  excel_result = []\n",
        "\n",
        "  # Iterate through each title and perform sentiment analysis\n",
        "  start_time = time.time()\n",
        "  for title in titles:\n",
        "      # Get the sentiment scores\n",
        "      sentiment_scores = sia.polarity_scores(title)\n",
        "\n",
        "      # Determine sentiment type and confidence\n",
        "      compound_score = sentiment_scores['compound']\n",
        "      if compound_score >= 0.05:\n",
        "          generalized_sentiment = 'positive'\n",
        "      elif compound_score <= -0.05:\n",
        "          generalized_sentiment = 'negative'\n",
        "      else:\n",
        "          generalized_sentiment = 'neutral'\n",
        "\n",
        "      # Confidence interpretation\n",
        "      confidence = abs(compound_score)  # The absolute value of the compound score as confidence\n",
        "\n",
        "      # Print the result in the specified format\n",
        "      # print(title)\n",
        "      # print(f\"{{'label': '{generalized_sentiment}', 'score': {confidence:.4f}}}\")\n",
        "      # print(\"-------------------------\")\n",
        "      excel_result.append((title, generalized_sentiment, confidence))\n",
        "\n",
        "\n",
        "  end_time = time.time()\n",
        "  duration = end_time - start_time\n",
        "\n",
        "  write_to_excel(excel_result, \"VADER\", excel_path)\n",
        "\n",
        "  return duration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm6kOUSNu3Bl"
      },
      "source": [
        "### 3. fine-tuned [DistilRoBERTa](https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis) (Transformers!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6vfiKhUzRkl"
      },
      "source": [
        "This is a transformer model that has been fine-tuned on the [financial phrasebank](https://huggingface.co/datasets/financial_phrasebank) dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWqA-TUhvMCy"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def distilroberta(titles, excel_path):\n",
        "  MODEL = \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
        "  pipe = pipeline(\"text-classification\", model=MODEL)\n",
        "  excel_result = []\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  for title in numerical_titles:\n",
        "    # print(title)\n",
        "    # print(pipe(title))\n",
        "    # print(\"-------------------------\")\n",
        "    data = pipe(title)\n",
        "    excel_result.append((title, data[0]['label'], data[0]['score']))\n",
        "\n",
        "  end_time = time.time()\n",
        "  duration = end_time - start_time\n",
        "\n",
        "  write_to_excel(excel_result, \"DistilRoBERTa\", excel_path)\n",
        "\n",
        "  return duration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLKUctQvRKra"
      },
      "source": [
        "### 4. [FinBERT](https://huggingface.co/yiyanghkust/finbert-tone) - 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98BINDGBPCVS"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "HF_TOKEN = \"HF_TOKEN\"\n",
        "\n",
        "def finbert(titles, excel_path):\n",
        "  finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
        "  tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
        "\n",
        "  sentiment_analysis = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
        "\n",
        "  titles = [str(title) for title in titles]\n",
        "  sentiment_results = sentiment_analysis(titles)\n",
        "\n",
        "  start_time = time.time()\n",
        "  excel_result = []\n",
        "\n",
        "  for title in titles:\n",
        "    sentiment_result = sentiment_analysis(title)\n",
        "    # print(title)\n",
        "    # print(sentiment_result)\n",
        "    # print(\"-------------------------\")\n",
        "    excel_result.append((title, sentiment_result[0]['label'], sentiment_result[0]['score']))\n",
        "\n",
        "  end_time = time.time()\n",
        "  duration = end_time - start_time\n",
        "\n",
        "  write_to_excel(excel_result, \"FinBERT\", excel_path)\n",
        "\n",
        "  return duration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSujRdgoSS02"
      },
      "source": [
        "### 5. [BART Large Mnli](https://huggingface.co/facebook/bart-large-mnli)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oB2MTlfySZS5"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQWx0GpxYukc"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def bart(titles, excel_path):\n",
        "  # Initialize the zero-shot classification pipeline\n",
        "  classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "  # Candidate labels for sentiment analysis\n",
        "  candidate_labels = [\"positive\", \"negative\", \"neutral\"]\n",
        "\n",
        "  # Classify the sentiment of each title\n",
        "  start_time = time.time()\n",
        "  excel_result = []\n",
        "\n",
        "  for title in numerical_titles:\n",
        "      # Perform zero-shot classification\n",
        "      result = classifier(title, candidate_labels)\n",
        "      sentiment = result['labels'][0]  # The label with the highest score\n",
        "      confidence = result['scores'][0]  # The confidence of the top label\n",
        "\n",
        "      # Print the result\n",
        "      # print(title)\n",
        "      # print(f\"[{{'label': '{sentiment}', 'score': {confidence:.4f}}}]\")\n",
        "      # print(\"-------------------------\")\n",
        "      excel_result.append((title, sentiment, confidence))\n",
        "\n",
        "  end_time = time.time()\n",
        "  duration = end_time - start_time\n",
        "\n",
        "  write_to_excel(excel_result, \"BART Large Mini\", excel_path)\n",
        "\n",
        "  return duration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYBKvxPwEe6i"
      },
      "source": [
        "### 6. [BERT](https://huggingface.co/docs/transformers/model_doc/bert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXbCp0QjFIiU"
      },
      "source": [
        "since BERT itself doesn't natively support zero-shot classification in the way models like facebook/bart-large-mnli do, we'll use a model that has been fine-tuned for sequence classification tasks and can infer sentiment directly. One such model is [nlptown/bert-base-multilingual-uncased-sentiment](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment), which can classify text into positive, negative, or neutral sentiments and is available on the Hugging Face model hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKtbYf9tEeQ4"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def bert(titles, excel_path):\n",
        "  # Initialize the sentiment analysis pipeline with a BERT model fine-tuned for sentiment\n",
        "  # Here we use 'nlptown/bert-base-multilingual-uncased-sentiment' which outputs sentiment scores\n",
        "  classifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "  excel_result = []\n",
        "\n",
        "  # Classify the sentiment of each title\n",
        "  start_time = time.time()\n",
        "\n",
        "  for title in numerical_titles:\n",
        "      # Perform sentiment analysis\n",
        "      results = classifier(title)\n",
        "\n",
        "      # The 'nlptown/bert-base-multilingual-uncased-sentiment' model provides labels and scores directly\n",
        "      sentiment = results[0]['label']\n",
        "      # Convert labels provided by this model into a more general form (positive, negative, neutral)\n",
        "      if sentiment in [\"1 star\", \"2 stars\"]:\n",
        "          generalized_sentiment = \"negative\"\n",
        "      elif sentiment in [\"4 stars\", \"5 stars\"]:\n",
        "          generalized_sentiment = \"positive\"\n",
        "      else:\n",
        "          generalized_sentiment = \"neutral\"\n",
        "      confidence = results[0]['score']\n",
        "\n",
        "      # Print the result\n",
        "      # print(title)\n",
        "      # print(f\"[{{'label': '{generalized_sentiment}', 'score': {confidence:.4f}}}]\")\n",
        "      # print(\"-------------------------\")\n",
        "      excel_result.append((title, generalized_sentiment, confidence))\n",
        "\n",
        "  end_time = time.time()\n",
        "  duration = end_time - start_time\n",
        "\n",
        "  write_to_excel(excel_result, \"BERT\", excel_path)\n",
        "\n",
        "  return duration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL4db5HC5Enr"
      },
      "source": [
        "## Conducting sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q2udOxN5mdN"
      },
      "outputs": [],
      "source": [
        "# 1. Textblob\n",
        "numerical_textblob_duration = textblob(numerical_titles, numerical_excel_path)\n",
        "contextual_textblob_duration = textblob(contextual_titles, contextual_excel_path)\n",
        "print(f\"numerical_textblob_duration: {numerical_textblob_duration} seconds\")\n",
        "print(f\"contextual_textblob_duration: {contextual_textblob_duration} seconds\")\n",
        "\n",
        "# 2. VADER\n",
        "numerical_vader_duration = vader(numerical_titles, numerical_excel_path)\n",
        "contextual_vader_duration = vader(contextual_titles, contextual_excel_path)\n",
        "print(f\"numerical_vader_duration: {numerical_vader_duration} seconds\")\n",
        "print(f\"contextual_vader_duration: {contextual_vader_duration} seconds\")\n",
        "\n",
        "# 3. DistilRoBERTa\n",
        "numerical_distilroberta_duration = distilroberta(numerical_titles, numerical_excel_path)\n",
        "contextual_distilroberta_duration = distilroberta(contextual_titles, contextual_excel_path)\n",
        "print(f\"numerical_distilroberta_duration: {numerical_distilroberta_duration} seconds\")\n",
        "print(f\"contextual_distilroberta_duration: {contextual_distilroberta_duration} seconds\")\n",
        "\n",
        "# 4. FinBERT\n",
        "numerical_finbert_duration = finbert(numerical_titles, numerical_excel_path)\n",
        "contextual_finbert_duration = finbert(contextual_titles, contextual_excel_path)\n",
        "print(f\"numerical_finbert_duration: {numerical_finbert_duration} seconds\")\n",
        "print(f\"contextual_finbert_duration: {contextual_finbert_duration} seconds\")\n",
        "\n",
        "# 5. BART Large Mini\n",
        "numerical_bart_duration = bart(numerical_titles, numerical_excel_path)\n",
        "contextual_bart_duration = bart(contextual_titles, contextual_excel_path)\n",
        "print(f\"numerical_bart_duration: {numerical_bart_duration} seconds\")\n",
        "print(f\"contextual_bart_duration: {contextual_bart_duration} seconds\")\n",
        "\n",
        "# 6. BERT\n",
        "numerical_bert_duration = bert(numerical_titles, numerical_excel_path)\n",
        "contextual_bert_duration = bert(contextual_titles, contextual_excel_path)\n",
        "print(f\"numerical_bert_duration: {numerical_bert_duration} seconds\")\n",
        "print(f\"contextual_bert_duration: {contextual_bert_duration} seconds\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "AyUh2f0TNdH8",
        "p3aRaaCMzMu6",
        "J359EVBvyfKN",
        "xrlhPyHEAMmQ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
